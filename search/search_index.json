{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Jake Notes","text":"<p>A collection of mathematical notes and machine learning articles.</p> <ul> <li> <p> Probability &amp; Bayesian Inference</p> <p>Beta-binomial distributions, hypergeometric sampling, and conjugate priors for ballot counting.</p> <p> Read more</p> </li> <li> <p> Linear Models &amp; ANOVA</p> <p>OLS derivation, ANOVA F-test geometry, R\u00b2 as squared correlation, and projection-based proofs.</p> <p> Read more</p> </li> <li> <p> Universal Algebra</p> <p>Operational types, free structures, algebraic theories, and equational reasoning from Johnstone's notes.</p> <p> Read more</p> </li> <li> <p> Attention Mechanisms</p> <p>Self-attention, positional encoding, compatibility functions, and multi-head attention in transformers.</p> <p> Read more</p> </li> <li> <p> CLIP &amp; SigLIP Losses</p> <p>Contrastive learning losses: softmax-based CLIP (InfoNCE) vs sigmoid-based SigLIP with learned bias.</p> <p> Read more</p> </li> </ul>"},{"location":"about/","title":"About","text":"<p>These are personal notes on mathematics and machine learning, written by Jake Baker.</p> <p>The notes cover topics in probability, linear models, abstract algebra, and deep learning. They are intended as a reference and study aid rather than a polished textbook.</p> <p>Built with Material for MkDocs.</p>"},{"location":"deep_learning/attention/","title":"Attention Transformer Details","text":""},{"location":"deep_learning/attention/#notation","title":"Notation","text":"<p>Notation</p> <p>Let \\((S, D)\\) denote the shape \\(\\mathbb{R}^{S \\times D}\\) and write \\(\\mathbf{X} \\in \\mathbb{R}^{S \\times D}\\) as \\(\\mathbf{X} : (S, D)\\).</p> <ul> <li>\\(S, T\\) denote sequence lengths (source and target respectively)</li> <li>\\(D\\) denotes the model embedding dimension</li> <li>\\(D_k, D_v, D_q\\) denote key, value, and query dimensions</li> <li>\\(h\\) denotes the number of attention heads, with \\(D_h \\coloneqq D/h\\)</li> </ul>"},{"location":"deep_learning/attention/#input-embeddings","title":"Input Embeddings","text":"<p>The raw text is a sequence of vocabulary terms. Tokenization is the process of splitting the sentence into a sequence of vocabulary terms. Vocab terms can be single words but often tokenizers split the sentences into smaller components than words. In illustrative examples we usually show tokens as words.</p> <p>Input Embeddings</p> <p>\\(S=5\\), \\(D\\) is usually 512 in the traditional paper.</p> \\[ \\begin{align*} \\begin{bmatrix} \\text{THE} \\\\ \\text{WEATHER} \\\\ \\text{IS} \\\\ \\text{LOVELY} \\\\ \\text{TODAY} \\end{bmatrix} &amp;\\mapsto \\begin{bmatrix} 105 \\\\ 12 \\\\ 24 \\\\ 107 \\\\ 57 \\end{bmatrix} &amp;&amp;\\mapsto \\begin{bmatrix} \\mathbf{x_1^T} \\\\ \\mathbf{x_2^T} \\\\ \\mathbf{x_3^T} \\\\ \\mathbf{x_4^T} \\\\ \\mathbf{x_5^T} \\end{bmatrix} \\\\ \\\\ \\text{Sentence} &amp;\\mapsto \\text{Input IDs} &amp;&amp;\\mapsto \\text{Embedding}:(5,D) \\end{align*} \\]"},{"location":"deep_learning/attention/#positional-encoding","title":"Positional Encoding","text":"<p>Fixed definition for each position \\(p \\in \\left\\{1,...,S \\right\\}\\) and dimension index \\(2i \\text{ or } 2i+1 \\text{ for } i \\in \\left\\{1,...,D \\right\\}\\).</p> \\[ PE :(S,D)  = \\begin{cases} PE_{p, 2i}     &amp;=&amp;   \\sin \\left( \\dfrac{p}{10,000^{\\left(\\dfrac{2i}{D}\\right)}} \\right) \\\\ \\\\ PE_{p, 2i+1}   &amp;=&amp;   \\cos \\left( \\dfrac{p}{10,000^{\\left(\\dfrac{2i}{D}\\right)}} \\right) \\end{cases} \\]"},{"location":"deep_learning/attention/#model-input","title":"Model Input","text":"<p>Model recieves \\(X + PE : (S,D)\\)</p>"},{"location":"deep_learning/attention/#self-attention","title":"Self Attention","text":"<p>This mechanism is what changed everything.</p>"},{"location":"deep_learning/attention/#attention-definition","title":"Attention Definition","text":"<p>Let Query, Key and Value Matricies be \\(\\mathbf{Q}:(T,D_k)\\), \\(\\mathbf{K}:(S,D_k)\\) and \\(\\mathbf{V}:(S,D_v)\\). We also let \\(T \\in \\mathbb{N}\\) denote a sequence length not neccessarily equal to \\(S \\in \\mathbb{N}\\).</p> <p>Attention</p> <p>\\(\\text{Attention}: (T,D_k) \\times (S,D_k) \\times (S,D_v) \\to (T,D_v)\\) maps</p> <p>$$ \\mathbf{Q,K,V} \\mapsto \\sigma \\left( \\dfrac{\\mathbf{Q K^T}}{\\sqrt{D_k}} \\right) \\mathbf{V} : (T,D_v) $$ for row wise softmax \\(\\sigma\\)</p>"},{"location":"deep_learning/attention/#explanation","title":"Explanation","text":"<p>Write the matricies as,</p> \\[ \\mathbf{Q}= \\begin{bmatrix}    \\mathbf{q_1^T}   \\\\    \\vdots   \\\\    \\mathbf{q_S^T} \\end{bmatrix} \\mathbf{K}= \\begin{bmatrix}    \\mathbf{k_1^T}   \\\\    \\vdots   \\\\    \\mathbf{k_S^T} \\end{bmatrix} \\mathbf{V}= \\begin{bmatrix}    \\mathbf{v_1^T}   \\\\    \\vdots   \\\\    \\mathbf{v_S^T} \\end{bmatrix} \\text{for } \\mathbf{q_i, k_i, v_i} \\in \\mathbb{R}^{D_v}, i=1,...,S \\] <p>We see that \\(\\mathbf{Z}_{i,j}\\coloneqq \\mathbf{QK^T}_{i,j} = \\mathbf{q_i \\cdotp k_j}\\) is like a similarity score between query \\(i\\) and key \\(j\\).</p>"},{"location":"deep_learning/attention/#compatibility-function","title":"Compatibility Function","text":"<p>If we observe the softmax applied to \\(\\mathbf{Z}\\), we see that each element is a compatibility function, \\(\\alpha\\), between query \\(i\\) and key \\(j\\).</p> \\[ \\begin{align*} \\sigma \\left( \\mathbf{Z} \\right)_{i,j} &amp;= \\cfrac{    \\exp \\left( \\frac{1}{\\sqrt{D_k}} \\mathbf{Z}_{i,j} \\right)    }    {\\sum_{r=1}^S \\exp \\left( \\frac{1}{\\sqrt{D_k}} \\mathbf{Z}_{i,r} \\right)    } \\\\ &amp;= \\cfrac{    \\exp \\left( \\frac{1}{\\sqrt{D_k}} \\mathbf{q_i \\cdotp k_j} \\right)    }    {\\sum_{r=1}^S \\exp \\left( \\frac{1}{\\sqrt{D_k}} \\mathbf{q_i \\cdotp k_r} \\right)    } = \\cfrac{\\text{score}(i,j)}{\\sum_{r=1}^S \\text{score}(i,r)} \\eqqcolon \\alpha(\\mathbf{q_i},\\mathbf{K},j) \\end{align*} \\] <p>Let \\(\\mathbf{A} \\coloneqq \\text{Attention}(\\mathbf{Q,K,V})\\), then</p> \\[ \\begin{align*} \\mathbf{A}_{i,j} = \\sum_{r=1}^S \\sigma \\left( \\mathbf{Z} \\right)_{i,r}  \\mathbf{V}_{r,j} &amp;= \\sum_{r=1}^S \\alpha(\\mathbf{q_i},\\mathbf{K},r) [\\mathbf{v_r}]_j \\\\ \\implies \\text{row}_i \\left( \\mathbf{A} \\right) &amp;= \\sum_{r=1}^S \\alpha(\\mathbf{q_i},\\mathbf{K},r) \\mathbf{v_r^T} \\in \\mathbb{R}^{1,D_v} \\end{align*} \\] <p>row \\(i\\) of \\(\\mathbf{A}\\) is the sum of values, each weighted by query \\(i\\)'s similarity with that key.</p>"},{"location":"deep_learning/attention/#remarks","title":"Remarks","text":"<p>Attention is permutation invariant</p> <p>Notice that row \\(i\\) of \\(\\mathbf{A}\\) is a function of \\(\\mathbf{q_i, K,V}\\),</p> \\[ \\text{row}_i \\left( \\mathbf{A} \\right) = \\sum_{r=1}^S \\alpha(\\mathbf{q_i},\\mathbf{K},r) \\mathbf{v_r^T} \\eqqcolon f(\\mathbf{q_i, K,V}) \\] <p>Therefore if we permute two rows of A, the output of Attention has the same two rows permuted.</p>"},{"location":"deep_learning/attention/#muti-head-attention","title":"Muti Head Attention","text":"<p>Let \\(\\mathbf{Q}:(T,D_q), \\mathbf{K}:(S,D_k), \\mathbf{V}:(S,D_v)\\).</p> <p>Multi Head Attention</p> <p>Given sequence lengths \\(S, T \\in \\mathbb{N}\\), dimension lengths \\(D_q, D_k, D_v, D \\in \\mathbb{N}\\), \\(h\\in \\mathbb{N}\\) that divides \\(D\\) and parameters \\(\\mathbf{W_i^Q, W_i^K, W_i^V}\\), for \\(i=1,...,h\\).</p> <p>\\(\\text{MultiHeadAttention}: (T,D_q) \\times (S,D_k) \\times (S,D_v) \\to (T,D)\\) maps</p> \\[ \\begin{split} \\mathbf{Q,K,V}  \\mapsto \\left[ \\mathbf{H_1},...,\\mathbf{H_h} \\right] : (T,D) \\\\ \\\\ \\mathbf{H_i}  \\coloneqq \\text{Attention} \\left( \\mathbf{Q'_i}, \\mathbf{K'_i}, \\mathbf{V'_i} \\right) :(T,D_h) \\\\ \\\\ \\begin{array}{ccccc}    \\mathbf{Q'_i} &amp;  = &amp; \\mathbf{Q} &amp; \\mathbf{W_i^Q} &amp;:(T,D_h), \\\\    &amp; &amp; (T,D_q) &amp; (D_q,D_h) &amp;    \\\\ \\\\    \\mathbf{K'_i} &amp;  = &amp; \\mathbf{K} &amp; \\mathbf{W_i^K} &amp;:(S,D_h) \\\\    &amp; &amp; (S,D_k) &amp; (D_k,D_h) &amp;    \\\\ \\\\    \\mathbf{V'_i} &amp;  = &amp; \\mathbf{V} &amp; \\mathbf{W_i^V} &amp;:(S,D_h) \\\\    &amp; &amp; (S,D_v) &amp; (D_v,D_h) &amp;    \\\\ \\end{array} \\\\ i=1,...,h, \\quad D_h \\coloneqq \\frac{D}{h} \\end{split} \\] <p>In practise we do the matrix multiplication all in one and then split.</p> \\[ \\begin{array}{llllll} \\mathbf{W^Q} &amp;= &amp;[\\mathbf{W_1^Q}, &amp;...,  &amp;\\mathbf{W_h^Q} ] &amp;:(D_q,D) \\\\ &amp;&amp;(D_q,D_h) &amp;   &amp;(D_q,D_h) &amp; \\\\ \\\\ \\mathbf{QW^Q} &amp;= &amp;[\\mathbf{QW_1^Q}, &amp;...,  &amp;\\mathbf{QW_h^Q} ] &amp;:(T,D) \\\\ &amp;&amp;(T,D_h) &amp;   &amp;(T,D_h) &amp; \\\\ \\\\ \\hline \\\\ \\mathbf{W^K} &amp;= &amp;[\\mathbf{W_1^K}, &amp;...,  &amp;\\mathbf{W_h^K} ] &amp;:(D_k,D) \\\\ &amp;&amp;(D_k,D_h) &amp;   &amp;(D_k,D_h) &amp; \\\\ \\\\ \\mathbf{KW^K} &amp;= &amp;[\\mathbf{KW_1^K}, &amp;...,  &amp;\\mathbf{KW_h^K} ] &amp;:(S,D) \\\\ &amp;&amp;(S,D_h) &amp;   &amp;(S,D_h) &amp; \\\\ \\\\ \\hline \\\\ \\mathbf{W^V} &amp;= &amp;[\\mathbf{W_1^V}, &amp;...,  &amp;\\mathbf{W_h^V} ] &amp;:(D_v,D) \\\\ &amp;&amp;(D_v,D_h) &amp;   &amp;(D_v,D_h) &amp; \\\\ \\\\ \\mathbf{VW^V} &amp;= &amp;[\\mathbf{VW_1^V}, &amp;...,  &amp;\\mathbf{VW_h^V} ] &amp;:(S,D) \\\\ &amp;&amp;(S,D_h) &amp;   &amp;(S,D_h) &amp; \\end{array} \\] <p>The general idea with different heads is for different heads to learn different roles that words can play.</p>"},{"location":"deep_learning/attention/#attention-visualisation","title":"Attention Visualisation","text":"<p>The diagrams in the paper are show the similarity between words according to \\(\\sigma \\left( \\frac{\\mathbf{QK^T}}{\\sqrt{D_k}} \\right)\\) of each head.</p>"},{"location":"deep_learning/clip_losses/","title":"CLIP & SigLIP Losses","text":""},{"location":"deep_learning/clip_losses/#loss-comparison-clip-vs-siglip","title":"Loss Comparison: CLIP vs SigLIP","text":"<p>Let \\(\\mathbf{U}, \\mathbf{V} \\in \\mathbb{R}^{N \\times D}\\) be the \\(\\ell_2\\)-normalized image and text embeddings respectively.</p> <p>Let \\(\\mathbf{S} = \\mathbf{U}\\mathbf{V}^\\top\\) be the \\(N \\times N\\) matrix of cosine similarity scores, where \\(s_{ij} = \\mathbf{u}_i \\cdot \\mathbf{v}_j\\).</p> <p>Let \\(t = e^{t'}\\) be the learned temperature parameter.</p>"},{"location":"deep_learning/clip_losses/#clip-loss","title":"CLIP Loss","text":"<p>For a vector \\(\\mathbf{x} \\in \\mathbb{R}^K\\), the softmax function is defined as:</p> \\[ \\text{Softmax}(\\mathbf{x})_k = \\frac{e^{x_k}}{\\sum_{c=1}^{K} e^{x_c}}, \\qquad k = 1, \\ldots, K \\] <p>Defining the row-wise softmax probabilities:</p> \\[ \\hat{\\mathbf{P}}^{(I \\to T)} = \\text{row-Softmax}(t \\cdot \\mathbf{S}), \\qquad \\hat{\\mathbf{P}}^{(T \\to I)} = \\text{row-Softmax}(t \\cdot \\mathbf{S}^\\top) \\] <p>That is,</p> \\[ \\hat{P}^{(I \\to T)}_{ij} = \\frac{e^{t \\cdot s_{ij}}}{\\sum_{c=1}^{N} e^{t \\cdot s_{ic}}} \\qquad \\hat{P}^{(T \\to I)}_{ij} = \\frac{e^{t \\cdot s_{ji}}}{\\sum_{c=1}^{N} e^{t \\cdot s_{ci}}} \\] <p>In general, for soft labels \\(y_{ij} \\in [0,1]\\) representing the affinity between image \\(i\\) and text \\(j\\):</p> \\[ \\begin{align*} \\mathcal{L}_{\\text{CLIP}}(\\mathbf{U}, \\mathbf{V}) &amp;= \\frac{1}{2} \\left[ \\mathbf{CE} \\left( \\hat{\\mathbf{P}}^{(I \\to T)}, \\mathbf{Y} \\right) + \\mathbf{CE} \\left( \\hat{\\mathbf{P}}^{(T \\to I)}, \\mathbf{Y} \\right) \\right] \\\\ &amp;= -\\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{N} y_{ij} \\left[ \\log \\hat{P}^{(I \\to T)}_{ij} + \\log \\hat{P}^{(T \\to I)}_{ij} \\right] \\end{align*} \\] <p>Where \\(\\mathbf{CE}\\) is the cross entropy function. For probability vectors \\(\\hat{\\mathbf{p}}, \\mathbf{y} \\in \\mathbb{R}^K\\) over \\(K\\) categories:</p> \\[ \\text{CE}(\\hat{\\mathbf{p}}, \\mathbf{y}) = -\\sum_{k=1}^{K} y_k \\log \\hat{p}_k \\] <p>For distribution matrices \\(\\hat{\\mathbf{P}} \\in \\mathbb{R}^{N \\times K}\\) and \\(\\mathbf{Y} \\in \\mathbb{R}^{N \\times K}\\), where each row is a distribution over \\(K\\) categories and \\(N\\) is the batch size:</p> \\[ \\mathbf{CE}(\\hat{\\mathbf{P}}, \\mathbf{Y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\text{CE}(\\hat{\\mathbf{P}}_{i,:},\\; \\mathbf{Y}_{i,:}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log \\hat{P}_{ik} \\] <p>In standard CLIP with hard labels:</p> <ul> <li>\\(y_{ij} = 1 \\iff\\) image \\(i\\) corresponds to text \\(j\\)</li> <li>\\(y_{ij} = 0 \\iff\\) image \\(i\\) does not correspond to text \\(j\\)</li> </ul> <p>The loss becomes:</p> \\[ \\begin{align*} \\mathcal{L}_{\\text{CLIP}}(\\mathbf{U}, \\mathbf{V}) &amp;= -\\frac{1}{2N} \\sum_{i=1}^{N} \\left[ \\log \\hat{P}^{(I \\to T)}_{ii} + \\log \\hat{P}^{(T \\to I)}_{ii} \\right] \\\\ &amp;= -\\frac{1}{2N} \\sum_{i=1}^{N} \\left[ \\underbrace{\\log \\frac{e^{t \\cdot s_{ii}}}{\\sum_{j=1}^{N} e^{t \\cdot s_{ij}}}}_{\\text{image} \\to \\text{text}} + \\underbrace{\\log \\frac{e^{t \\cdot s_{ii}}}{\\sum_{j=1}^{N} e^{t \\cdot s_{ji}}}}_{\\text{text} \\to \\text{image}} \\right] \\end{align*} \\] <p>Hence, CLIP uses a symmetric softmax-based contrastive loss (InfoNCE). For each image \\(i\\), it treats the matching text \\(i\\) as the correct class among \\(N\\) candidates (and vice versa).</p>"},{"location":"deep_learning/clip_losses/#siglip-loss","title":"SigLIP Loss","text":"<p>Defining the pairwise sigmoid probability:</p> \\[ \\hat{P}^{\\text{sig}}_{ij} = \\sigma(t \\cdot s_{ij} - b) = \\frac{1}{1 + e^{-(t \\cdot s_{ij} - b)}} \\] <p>where \\(b\\) is a learned bias term.</p> <p>In general, for soft labels \\(y_{ij} \\in [0,1]\\) representing the affinity between image \\(i\\) and text \\(j\\):</p> <p>Note that binary cross entropy (BCE) is simply the \\(C = 2\\) case of CE. For a single prediction \\(\\hat{p} \\in [0,1]\\) and label \\(y \\in \\{0,1\\}\\), the distribution vectors are \\(\\hat{\\mathbf{p}} = (\\hat{p},\\; 1 - \\hat{p})\\) and \\(\\mathbf{y} = (y,\\; 1 - y)\\), giving:</p> \\[ \\text{BCE}(\\hat{p}, y) = \\text{CE}\\!\\left((\\hat{p},\\; 1 - \\hat{p}),\\; (y,\\; 1 - y)\\right) = -\\left[ y \\log \\hat{p} + (1 - y) \\log(1 - \\hat{p}) \\right] \\] <p>Each pair \\((i, j)\\) contributes one such binary classification. Applying this to every pair:</p> \\[ \\begin{align*} \\mathcal{L}_{\\text{SigLIP}}(\\mathbf{U}, \\mathbf{V}) &amp;= \\frac{1}{N^2} \\sum_{(i,j) \\in \\{1,\\ldots,N\\}^2} \\text{BCE}\\!\\left(\\hat{P}^{\\text{sig}}_{ij},\\; y_{ij}\\right) \\\\ &amp;= -\\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\left[ y_{ij} \\log \\hat{P}^{\\text{sig}}_{ij} + (1 - y_{ij}) \\log \\left(1 - \\hat{P}^{\\text{sig}}_{ij}\\right) \\right] \\end{align*} \\] <p>Equivalently, flattening the \\((i,j)\\) pairs into a single batch dimension of size \\(N^2\\), this is just the matrix \\(\\mathbf{CE}\\) applied to an \\(N^2 \\times 2\\) matrix where each row contains \\((\\hat{P}^{\\text{sig}}_{ij},\\; 1 - \\hat{P}^{\\text{sig}}_{ij})\\) with corresponding labels \\((y_{ij},\\; 1 - y_{ij})\\):</p> \\[ \\mathcal{L}_{\\text{SigLIP}}(\\mathbf{U}, \\mathbf{V}) = \\mathbf{CE}\\!\\left(\\hat{\\mathbf{P}}_{\\text{flat}},\\; \\mathbf{Y}_{\\text{flat}}\\right), \\qquad \\hat{\\mathbf{P}}_{\\text{flat}}, \\mathbf{Y}_{\\text{flat}} \\in \\mathbb{R}^{N^2 \\times 2} \\] <p>In standard SigLIP with hard labels:</p> <ul> <li>\\(y_{ij} = 1 \\iff\\) image \\(i\\) corresponds to text \\(j\\) (positive pair)</li> <li>\\(y_{ij} = 0 \\iff\\) image \\(i\\) does not correspond to text \\(j\\) (negative pair)</li> </ul> <p>The loss becomes:</p> \\[ \\begin{align*} \\mathcal{L}_{\\text{SigLIP}}(\\mathbf{U}, \\mathbf{V}) &amp;= -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\log \\hat{P}^{\\text{sig}}_{ii} + \\sum_{j \\neq i} \\log \\left(1 - \\hat{P}^{\\text{sig}}_{ij}\\right) \\right] \\\\ &amp;= -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\log \\sigma\\left( z_{ij} \\cdot (t \\cdot s_{ij} - b) \\right) \\end{align*} \\] <p>where we use \\(z_{ij} = 2y_{ij} - 1 \\in \\{+1, -1\\}\\) and the identity \\(1 - \\sigma(x) = \\sigma(-x)\\).</p> <p>Learnable parameters:</p> <ul> <li>\\(t = e^{t'}\\): temperature (initialized with \\(t' = \\log 10\\), so \\(t = 10\\))</li> <li>\\(b\\): bias term (initialized to \\(-10\\))</li> </ul> <p>The bias \\(b\\) is crucial for training stability \u2014 it prevents large initial gradient updates from the heavy imbalance of \\(N^2 - N\\) negative pairs vs \\(N\\) positive pairs.</p>"},{"location":"deep_learning/clip_losses/#key-differences","title":"Key Differences","text":"Property CLIP SigLIP Loss type Softmax (N-way classification) Sigmoid (pairwise binary) Normalization Global across batch Per-pair independent Symmetry Two passes (I\u2192T and T\u2192I) Single symmetric pass Learnable params Temperature \\(t\\) Temperature \\(t\\) + bias \\(b\\) Memory \\(O(N^2)\\) for full softmax \\(O(N^2)\\) but chunked-friendly"},{"location":"deep_learning/clip_losses/#clip-loss-multiclass-analysis","title":"CLIP Loss Multiclass Analysis","text":"<p>When a batch contains \\(n\\) images \\(i, i{+}1, \\ldots, i{+}n{-}1\\) whose corresponding texts are all identical, i.e. \\(\\mathbf{v}_{i} = \\mathbf{v}_{i+1} = \\cdots = \\mathbf{v}_{i+n-1}\\), it is natural to replace the identity target \\(\\mathbf{Y} = \\mathbf{I}_N\\) with a block-diagonal target \\(\\mathbf{Y}'\\) that assigns equal credit to every correct match. Concretely, \\(\\mathbf{Y}'\\) equals \\(\\mathbf{I}_N\\) except on the \\(n \\times n\\) block covering rows and columns \\(i, \\ldots, i{+}n{-}1\\), where:</p> \\[ Y'_{i+k,\\, i+c} = \\frac{1}{n}, \\qquad k, c \\in \\{0, \\ldots, n-1\\} \\] <p>We prove that this does not change the loss: \\(\\mathcal{L}_{\\text{CLIP}}(\\mathbf{Y}) = \\mathcal{L}_{\\text{CLIP}}(\\mathbf{Y}')\\).</p> <p>Since \\(\\mathbf{Y}\\) and \\(\\mathbf{Y}'\\) differ only on rows \\(i, \\ldots, i{+}n{-}1\\), it suffices to show equality of the contributions from these rows in both the image-to-text and text-to-image terms.</p>"},{"location":"deep_learning/clip_losses/#key-identity","title":"Key identity","text":"<p>Because the texts agree, \\(\\mathbf{v}_{i+c} = \\mathbf{v}_{i+k}\\), we have \\(s_{r,\\, i+c} = s_{r,\\, i+k}\\) for all \\(r\\) and all \\(c, k \\in \\{0, \\ldots, n{-}1\\}\\). Since the softmax is applied row-wise to \\(t \\cdot \\mathbf{S}\\), this gives:</p> \\[ \\hat{P}^{(I \\to T)}_{r,\\, i+c} = \\hat{P}^{(I \\to T)}_{r,\\, i+k} \\qquad \\forall\\; r, \\quad \\forall\\; c, k \\in \\{0, \\ldots, n{-}1\\} \\tag{$\\ast$} \\] <p>That is, for every row \\(r\\), the entries at columns \\(i\\) through \\(i{+}n{-}1\\) are equal (same-coloured entries denote equal values):</p> \\[ \\hat{\\mathbf{P}}^{(I \\to T)} = \\begin{array}{c c}   &amp; \\begin{array}{cccccccc}     \\scriptstyle T_1 &amp; \\scriptstyle \\cdots &amp; \\scriptstyle T_i &amp; \\scriptstyle T_{i{+}1} &amp; \\scriptstyle \\cdots &amp; \\scriptstyle T_{i{+}n{-}1} &amp; \\scriptstyle \\cdots &amp; \\scriptstyle T_N   \\end{array} \\\\   \\begin{array}{c}     \\scriptstyle I_1 \\\\ \\scriptstyle \\vdots \\\\ \\scriptstyle I_r \\\\ \\scriptstyle \\vdots \\\\ \\scriptstyle I_N   \\end{array} &amp;   \\left[\\begin{array}{cccccccc}     \\cdots &amp; \\cdots &amp; \\color{red}{\\bullet} &amp; \\color{red}{\\bullet} &amp; \\color{red}{\\cdots} &amp; \\color{red}{\\bullet} &amp; \\cdots &amp; \\cdots \\\\     &amp; &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; &amp; \\\\     \\cdots &amp; \\cdots &amp; \\color{green}{\\bullet} &amp; \\color{green}{\\bullet} &amp; \\color{green}{\\cdots} &amp; \\color{green}{\\bullet} &amp; \\cdots &amp; \\cdots \\\\     &amp; &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; &amp; \\\\     \\cdots &amp; \\cdots &amp; \\color{purple}{\\bullet} &amp; \\color{purple}{\\bullet} &amp; \\color{purple}{\\cdots} &amp; \\color{purple}{\\bullet} &amp; \\cdots &amp; \\cdots   \\end{array}\\right] \\end{array} \\] <p>Similarly, since the text-to-image term \\(\\hat{\\mathbf{P}}^{(T \\to I)} = \\text{row-Softmax}(t \\cdot \\mathbf{S}^\\top)\\).</p> \\[ \\hat{P}^{(T \\to I)}_{i+k,\\, r} = \\hat{P}^{(T \\to I)}_{i+c,\\, r} \\qquad \\forall\\; r, \\quad \\forall\\; c, k \\in \\{0, \\ldots, n{-}1\\} \\tag{$\\ast\\ast$} \\] <p>That is, rows \\(i\\) through \\(i{+}n{-}1\\) of \\(\\hat{\\mathbf{P}}^{(T \\to I)}\\) are identical:</p> \\[ \\hat{\\mathbf{P}}^{(T \\to I)} = \\begin{array}{c c}   &amp; \\begin{array}{ccccc}     \\scriptstyle I_1 &amp; \\scriptstyle \\cdots &amp; \\scriptstyle I_r &amp; \\scriptstyle \\cdots &amp; \\scriptstyle I_N   \\end{array} \\\\   \\begin{array}{c}     \\scriptstyle T_1 \\\\ \\scriptstyle \\vdots \\\\ \\scriptstyle T_i \\\\ \\scriptstyle T_{i{+}1} \\\\ \\scriptstyle \\vdots \\\\ \\scriptstyle T_{i{+}n{-}1} \\\\ \\scriptstyle \\vdots \\\\ \\scriptstyle T_N   \\end{array} &amp;   \\left[\\begin{array}{ccccc}     \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\     &amp; &amp; \\vdots &amp; &amp; \\\\     \\color{red}{\\bullet} &amp; \\color{orange}{\\cdots} &amp; \\color{green}{\\bullet} &amp; \\color{blue}{\\cdots} &amp; \\color{purple}{\\bullet} \\\\     \\color{red}{\\bullet} &amp; \\color{orange}{\\cdots} &amp; \\color{green}{\\bullet} &amp; \\color{blue}{\\cdots} &amp; \\color{purple}{\\bullet} \\\\     \\color{red}{\\vdots} &amp; &amp; \\color{green}{\\vdots} &amp; &amp; \\color{purple}{\\vdots} \\\\     \\color{red}{\\bullet} &amp; \\color{orange}{\\cdots} &amp; \\color{green}{\\bullet} &amp; \\color{blue}{\\cdots} &amp; \\color{purple}{\\bullet} \\\\     &amp; &amp; \\vdots &amp; &amp; \\\\     \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots   \\end{array}\\right] \\end{array} \\]"},{"location":"deep_learning/clip_losses/#part-1-image-to-text-loss","title":"Part 1: Image-to-text loss","text":"<p>Consider the contribution of row \\(i{+}k\\) for a fixed \\(k \\in \\{0, \\ldots, n{-}1\\}\\).</p> <p>Under \\(\\mathbf{Y} = \\mathbf{I}_N\\):</p> \\[ -\\log \\hat{P}^{(I \\to T)}_{i+k,\\, i+k} \\] <p>Under \\(\\mathbf{Y}'\\):</p> \\[ -\\sum_{c=0}^{n-1} \\frac{1}{n} \\log \\hat{P}^{(I \\to T)}_{i+k,\\, i+c} = -\\frac{1}{n} \\sum_{c=0}^{n-1} \\log \\hat{P}^{(I \\to T)}_{i+k,\\, i+k} = -\\log \\hat{P}^{(I \\to T)}_{i+k,\\, i+k} \\] <p>where the second equality uses \\((\\ast)\\). Since this holds for every \\(k\\), the image-to-text contribution from these \\(n\\) rows is unchanged.</p>"},{"location":"deep_learning/clip_losses/#part-2-text-to-image-loss","title":"Part 2: Text-to-image loss","text":"<p>Under \\(\\mathbf{Y}'\\):</p> <p>Under the block target, every row \\(i{+}k\\) in this block has both the same softmax outputs (by \\((\\ast\\ast)\\)) and the same target row (\\(Y'_{i+k,:}\\) is identical for all \\(k\\)). Therefore every row contributes the same loss, and the total contribution from the \\(n\\) rows is \\(n\\) times the loss from a single row:</p> \\[ n \\cdot \\left[ \\text{loss from row i} \\right] = n \\cdot \\left[ -\\sum_{c=0}^{n-1} \\frac{1}{n} \\log \\hat{P}^{(T \\to I)}_{i,\\, i+c} \\right] = -\\sum_{c=0}^{n-1} \\log \\hat{P}^{(T \\to I)}_{i,\\, i+c} \\] <p>Under \\(\\mathbf{Y} = \\mathbf{I}_N\\):</p> \\[ -\\sum_{k=0}^{n-1} \\log \\hat{P}^{(T \\to I)}_{i+k,\\, i+k} = -\\sum_{k=0}^{n-1} \\log \\hat{P}^{(T \\to I)}_{i,\\, i+k} \\] <p>where the second equality applies \\((\\ast\\ast)\\) to replace row \\(i{+}k\\) with row \\(i\\). This is the same expression.</p>"},{"location":"deep_learning/clip_losses/#conclusion","title":"Conclusion","text":"\\[ \\boxed{\\mathcal{L}_{\\text{CLIP}}(\\mathbf{Y}) = \\mathcal{L}_{\\text{CLIP}}(\\mathbf{Y}')} \\] <p>The duplicate texts create two symmetries. Column equality \\((\\ast)\\) makes the image-to-text block target simply average \\(n\\) identical log-probabilities. Row equality \\((\\ast\\ast)\\) lets the text-to-image contributions collapse: both the identity and block targets reduce to the same sum \\(-\\sum_{c=0}^{n-1} \\log \\hat{P}^{(T \\to I)}_{i,\\, i+c}\\). Consequently, assigning soft targets that split credit equally among duplicate text entries does not change the CLIP loss.</p> <p>So... if we want a multi class loss function we will have to get back to the drawing board!</p> <p>Note this proof works for duplicate images too by symetry.</p>"},{"location":"mathematics/linear_models/linear_models/","title":"Linear Model Notes","text":""},{"location":"mathematics/linear_models/linear_models/#ols-basics","title":"OLS Basics","text":"<p>Standard Normal Vector</p> <p>A random vector \\(\\mathbf{z} \\in \\mathbb{R^n}\\) is standard normal iff components \\((\\mathbf{z_i})_{i=1}^n\\) are independantly identically distributed \\(\\mathcal{N}(\\mathbf{0,I_n})\\).</p> <p>Note</p> <p>If a vector in standard normal w.r.t one basis then it is standard normal w.r.t any basis.</p> <p>OLS</p> <p>An ordinary least squares linear model (OLM) provides an estimate \\(\\hat{\\beta}\\) of unknown coefficent \\(\\beta \\in \\mathbb{R^{p \\times 1}}\\) to the problem,</p> \\[ \\mathbf{y} = \\mathbf{X} \\beta + \\mathbf{\\varepsilon} \\] <p>that minimises \\(\\| \\hat{\\varepsilon} \\|\\) for \\(\\hat{\\varepsilon} = \\mathbf{y-X \\hat{\\beta}}\\).</p> <p>Items \\(\\mathbf{y} \\in \\mathbb{R}^{n \\times 1}, \\mathbf{X} \\in \\mathbb{R^{n \\times p}}\\) are known.</p> <p>We assume that \\(\\dfrac{\\varepsilon}{\\sigma}\\) is standard normal (Definition) \\(\\Leftrightarrow \\varepsilon \\sim \\mathcal{N}(\\mathbf{0, \\sigma^2 I_n})\\), where we may not know \\(\\sigma\\).</p> <p>Solution of OLS</p> <p>Assume that \\(\\mathbf{X}\\) has full rank.</p> <p>The solution of OLS is given by \\(\\hat{\\beta} = \\mathbf{(X^TX)^{-1}X^Ty}\\).</p> Proof <p>Let subspace \\(U \\leq \\mathbb{R^n}\\) generated via the span of \\(\\mathbf{X}\\) columns. Then by property of orthogonal projection the \\(\\mathbf{\\hat{y}=X \\beta} \\in U\\) that minimises \\(\\| \\hat{\\varepsilon} \\| = \\| \\mathbf{y-\\hat{y}} \\|\\) is given by \\(\\mathbf{P}_U \\mathbf{y}\\).</p> <p>Therefore,</p> \\[ \\begin{align*} \\forall u \\in U, \\langle u, \\hat{\\varepsilon}\\rangle =0 &amp;\\implies \\mathbf{(y-X \\hat{\\beta})^TX} = 0 \\\\ &amp;\\Leftrightarrow \\mathbf{y^T X} = \\mathbf{\\hat{\\beta}^T X^TX} \\\\ &amp;\\Leftrightarrow \\mathbf{X^Ty} = \\mathbf{X^T X \\hat{\\beta}} \\\\ &amp;\\Leftrightarrow \\mathbf{\\hat{\\beta}} = \\mathbf{(X^TX)^{-1}X^Ty} \\end{align*} \\]"},{"location":"mathematics/linear_models/linear_models/#large-model-vs-submodel","title":"Large Model vs Submodel","text":"<p>ANOVA F-test</p> <p>Let \\(p=p_0+p_1, \\mathbf{X = [X_0, X_1]} \\in \\mathbb{R}^{n \\times p}\\) for \\(\\mathbf{X_0} \\in \\mathbb{R}^{n \\times p_0}, \\mathbf{X_1} \\in \\mathbb{R}^{n \\times p_1}\\)</p> <p>Consider the following two hypothesis of a full model vs a submodel,</p> \\[ \\begin{align*} \\textbf{H}_0 \\text{ (null)}&amp;: \\mathbf{y = X_0 \\beta_\\mathcal{N} + \\varepsilon}, \\quad &amp;&amp; \\mathbf{\\beta}_\\mathcal{N} \\in \\mathbb{R}^{p_0 \\times 1}, \\quad &amp;&amp;&amp;\\mathbf{\\varepsilon \\sim \\mathcal{N}(0,I_n)} \\\\ \\textbf{H}_1 \\text{ (full)}&amp;: \\mathbf{y = X \\beta_\\mathcal{F} + \\varepsilon}, \\quad &amp;&amp; \\mathbf{\\beta}_\\mathcal{F} \\in \\mathbb{R}^{p \\times 1}, \\quad &amp;&amp;&amp;\\mathbf{\\varepsilon \\sim \\mathcal{N}(0,I_n)} \\end{align*} \\] <p>Under the null hypothesis the following statistic is F distributed.</p> \\[ \\dfrac{\\| \\mathbf{\\hat{\\varepsilon}_\\mathcal{N} - \\hat{\\varepsilon}_\\mathcal{F}} \\| /p_1} {\\| \\mathbf{\\hat{\\varepsilon}_\\mathcal{F}} \\|/(n-p)} \\sim F_{p_1, n-p} \\] Proof <p>The following picture is helpful:</p> <p></p> <p>Visualisation of the geometry of the null and full model.</p> <p>Let \\(U_0, U\\) denote the column span of \\(\\mathbf{X_0, X}\\) respectively. Since \\(U_0 \\leq U\\) the orthogonal compliment of \\(U_0\\) restricted to \\(U\\) is in the direct sum \\(U = U_0 \\oplus U_0^{\\perp|_U}\\). Also \\(V = U \\oplus U^\\perp\\). Therefore \\(V\\) is the direct sum of orthogonal subspaces \\(U_0, U_1, U^\\perp\\), for \\(U_1 = U_0^{\\perp|_U}\\).</p> <p>Since the components of \\(\\mathbf{\\varepsilon}\\) are independant w.r.t any normal basis it olds that it holds that projections \\(\\mathbf{P}_{U_0} \\varepsilon, \\mathbf{P}_{U_1} \\varepsilon, \\mathbf{P}_{U^\\perp} \\varepsilon\\) are independant because each projection can be expressed in terms of disjoint elements of the orthnormal basis created by combining orthonormal basis of \\(U_0, U_1, U^\\perp\\).</p> <p>We will now show that \\(\\mathbf{\\hat{\\varepsilon}_\\mathcal{N} - \\hat{\\varepsilon}_\\mathcal{F}} = \\mathbf{P}_{U_1} \\varepsilon\\) and \\(\\mathbf{\\hat{\\varepsilon}_\\mathcal{F}} = \\mathbf{P}_{U^\\perp} \\varepsilon\\). This implies that they are independant normal and therefore the ratio of their norms divided by their dimension/degrees of freedom is eqivalent to an F distribution and thus completing the proof.</p> <p>We see,</p> <p>\\(\\mathbf{y} = \\mathbf{P}_U \\mathbf{y} + \\mathbf{P}_U^\\perp \\mathbf{y}\\) from directness,</p> <p>\\(\\mathbf{y} = \\mathbf{P}_U \\mathbf{y} + \\mathbf{\\hat{\\varepsilon}}_\\mathcal{F}\\) from OLS definition and the fact that full fitted \\(\\mathbf{y}\\) is given by \\(\\mathbf{P}_U \\mathbf{y}\\)</p> <p>Hence \\(\\mathbf{\\hat{\\varepsilon}}_\\mathcal{F} = \\mathbf{P}_{U^\\perp} \\mathbf{y}\\). Also,</p> <p>\\(\\mathbf{y} = \\mathbf{P}_{U_0} \\mathbf{y} + \\mathbf{P}_{U_1} \\mathbf{y} + \\mathbf{P}_{U^\\perp} \\mathbf{y}\\) from directness,</p> <p>\\(\\mathbf{y} = \\mathbf{P}_{U_0} \\mathbf{y} + \\mathbf{\\hat{\\varepsilon}}_\\mathcal{N}\\) from OLS definition and the fact that null fitted \\(\\mathbf{y}\\) is given by \\(\\mathbf{P}_{U_0} \\mathbf{y}\\)</p> <p>Hence \\(\\mathbf{\\hat{\\varepsilon}}_\\mathcal{N} = \\mathbf{P}_{U_1} \\mathbf{y} + \\mathbf{P}_{U^\\perp }\\mathbf{y}\\). Therefore \\(\\mathbf{\\hat{\\varepsilon}}_\\mathcal{N} - \\mathbf{\\hat{\\varepsilon}}_\\mathcal{F} = \\mathbf{P}_{U_1} \\mathbf{y}\\).</p> <p>Now under the null hypothesis \\(\\mathbf{y} = \\mu_\\mathcal{N} + \\varepsilon\\) for some \\(\\mu_\\mathcal{N} \\in U_0\\), therefore since \\(U_1, U^\\perp\\) are orthogonal to \\(U_0\\) it holds that for any \\(W \\in \\lbrace U_1, U^\\perp \\rbrace \\ \\mathbf{P}_W \\mathbf{y} = \\mathbf{P}_W \\mathbf{\\varepsilon}\\). This gives us what we wanted.</p>"},{"location":"mathematics/linear_models/linear_models/#r2-and-sample-correlation","title":"R2 and Sample Correlation","text":"<p>Sample Correlation</p> <p>The sample correlation between vectors \\(\\mathbf{x,y}\\) is defined as</p> \\[ \\begin{align*} r(\\mathbf{x,y}) &amp;= \\cfrac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{ \\sqrt{\\sum_i (x_i - \\bar{x})^2}  \\sqrt{\\sum_i (y_i - \\bar{y})^2}} \\\\ &amp;= \\cfrac{(\\mathbf{x-P_1 x})^T (\\mathbf{y-P_1 y})}{\\| \\mathbf{x-P_1 x} \\| \\| \\mathbf{y-P_1 y} \\|} \\\\ &amp;= \\bigg\\langle   \\cfrac{\\mathbf{P_{1^\\perp} x}}{\\|\\mathbf{P_{1^\\perp} x}\\|},   \\cfrac{\\mathbf{P_{1^\\perp} y}}{\\|\\mathbf{P_{1^\\perp} y}\\|} \\bigg\\rangle \\end{align*} \\] <p>Where we are lax with notation and let \\(\\mathbf{1}\\) denote \\(\\text{span}\\lbrace\\mathbf{1}\\rbrace\\) for \\(\\mathbf{1} = [1,\\ ...\\ ,1]^T\\).</p> <p>Coefficient of Determination</p> <p>Given real and predicted \\(\\mathbf{y}\\), \\(\\mathbf{\\hat{y}}\\), the coefficent of determination is defined as \\(R2(\\mathbf{y,\\hat{y}})\\)</p> \\[ \\begin{align*} &amp; R2(\\mathbf{y,\\hat{y}}) = 1 - \\frac{SS_{res}}{SS_{tot}}, &amp; \\text{for } SS_{res} &amp;= \\| \\mathbf{y-\\hat{y}} \\|^2 &amp; SS_{tot} &amp;= \\| \\mathbf{y-\\bar{y}} \\|^2 \\\\ &amp;&amp;&amp;= \\|\\varepsilon\\|^2 &amp;&amp;= \\mathbf{\\|P_{1^\\perp} y\\|^2} \\end{align*} \\] <p>Remark</p> <p>Note that \\(\\mathbf{\\hat{y}}\\) in Definition is general. We will specify details to prove the following.</p> <p>R\u00b2 as Squared Correlation</p> <p>If the predictions \\(\\mathbf{\\hat{y}}\\) are standard ols as in Definition, then it holds that</p> \\[ r(\\mathbf{y}, \\mathbf{\\hat{y}}) = R2(\\mathbf{y,\\hat{y}}) \\] Proof <p>Take \\(\\mathbf{y = \\hat{y}} + \\hat{\\varepsilon}\\), project into \\(\\mathbf{1^\\perp}\\) to get</p> <p>\\(\\mathbf{P_{1^\\perp}y} = \\mathbf{P_{1^\\perp}\\hat{y}} + \\hat{\\varepsilon}\\) since \\(\\hat{\\varepsilon} \\in U^\\perp \\leq 1^\\perp\\).</p> <p>Since \\(\\mathbf{\\hat{y}} \\in U\\) and \\(\\hat{\\varepsilon} \\in U^\\perp\\) are perpendicular the same therefore holds for \\(\\mathbf{P_{1^\\perp}\\hat{y}} \\in U \\cap \\mathbf{1^\\perp}\\) and \\(\\hat{\\varepsilon} \\in U^\\perp\\).</p> <p>The following picture summarises things well.</p> <p></p> <p>Note</p> <p>Note that for any right angled triangle the following holds:</p> \\[ [\\mathbf{b = c-a}] \\perp \\mathbf{a} \\implies \\langle \\mathbf{c,a} \\rangle = \\| \\mathbf{a} \\|^2 \\implies \\langle \\mathbf{\\frac{a}{\\|a\\|}, \\frac{c}{\\|c\\|}} \\rangle = \\mathbf{\\frac{\\|a\\|}{\\|c\\|}} \\] <p>So we have</p> \\[ \\begin{align*} r(\\mathbf{y}, \\mathbf{\\hat{y}}) &amp;= \\bigg\\langle   \\cfrac{\\mathbf{P_{1^\\perp} y}}{\\|\\mathbf{P_{1^\\perp} y}\\|},   \\cfrac{\\mathbf{P_{1^\\perp} \\hat{y}}}{\\|\\mathbf{P_{1^\\perp} \\hat{y}}\\|} \\bigg\\rangle = \\bigg\\langle   \\cfrac{\\mathbf{c}}{\\|\\mathbf{c}\\|},   \\cfrac{\\mathbf{a}}{\\|\\mathbf{a}\\|} \\bigg\\rangle = \\mathbf{\\frac{\\|a\\|}{\\|c\\|}} \\\\ &amp;= \\mathbf{\\frac{\\|P_{1^\\perp} \\hat{y}\\|}{\\|P_{1^\\perp} y\\|} } \\end{align*} \\] <p>and thus,</p> \\[ r(\\mathbf{y}, \\mathbf{\\hat{y}})^2 = \\mathbf{\\frac{\\|P_{1^\\perp} \\hat{y}\\|^2}{\\|P_{1^\\perp} y\\|^2} } = \\mathbf{\\frac{\\|P_{1^\\perp} y\\|^2 - \\|\\varepsilon\\|^2}{\\|P_{1^\\perp} y\\|^2} } = 1 - \\frac{SS_{res}}{SS_{tot}} \\]"},{"location":"mathematics/probability/ballot_notes/","title":"Ballot Sampling with Bayes","text":""},{"location":"mathematics/probability/ballot_notes/#beta-binomial","title":"Beta Binomial","text":"<p>Beta-Binomial Distribution</p> <p>A random variable \\(X\\) has a Beta-Binomial distribution, \\(X \\sim \\mathrm{BetaBin} (n,\\alpha ,\\beta)\\) iff</p> \\[ \\mathbb{P}(X=x) = f(x|n,\\alpha ,\\beta) = {n \\choose x} \\Gamma(x+\\alpha) \\Gamma(n-x + \\beta) \\] <p>for \\(x \\in \\lbrace 1,...,n \\rbrace\\).</p> <p>Remark</p> <p>Note that \\(f(x|n,\\alpha ,\\beta) \\propto \\frac{1}{x! (n-x)!} \\Gamma(x+\\alpha) \\Gamma(n-x + \\beta)\\) when we consider \\(x\\) to be the only variable.</p> <p>This distribution can be derived from a binomial likelihood with a beta distributed prior \\(\\theta=\\mathbb{P}(\\text{success})\\).</p>"},{"location":"mathematics/probability/ballot_notes/#hypergeometric-distribution","title":"Hypergeometric Distribution","text":"<p>Hypergeometric Distribution</p> <p>Suppose there are \\(N\\) balls, \\(X\\) of the balls are red. Suppose we sample \\(M\\) balls without replacement. Let \\(Y\\) denote the number of red balls in the sample. Then,</p> \\[ \\mathbb{P}(Y=y) = \\frac{{x \\choose y} {N-X \\choose M-y}}{N \\choose M} \\] Proof <p>As a result of the multiplication principle. The number of combinations that contain y red balls is \\({X \\choose y} \\times {N-X \\choose M-y}\\).</p>"},{"location":"mathematics/probability/ballot_notes/#conjugate-prior","title":"Conjugate Prior","text":"<p>Hypergeometric Conjugate Prior</p> <p>Suppose there are \\(N\\) ballots and we sample \\(M \\in \\lbrace 1, ..., N \\rbrace\\). Let the random variable \\(X\\) be the total number of Labour ballots. Let \\(Y\\) be the number of Labour ballots in the sample.</p> <p>If we place the following prior on \\(X\\),</p> \\[ p(x) = f(x|N,\\alpha ,\\beta) \\] <p>then likelihood of \\(Y\\) given \\(X\\) is hypergeometric and the posterior distribution of \\(X\\) is given by</p> \\[ p(x|y) = f(x-y|N-M, \\alpha+y, \\beta + M-y) \\] Proof <p>Notice that if we consider \\(x\\) the only variable,</p> \\[ \\begin{align*} f(\\textcolor{green}{x-y} | N-M, \\textcolor{blue}{\\alpha+y}, \\textcolor{red}{\\beta + M-y}) &amp; \\propto \\frac {\\Gamma(\\textcolor{green}{x-y} + \\textcolor{blue}{\\alpha+y}) \\Gamma(N-M \\textcolor{green}{-x+y} + \\textcolor{red}{\\beta + M-y})} {(\\textcolor{green}{x-y})! (N-M \\textcolor{green}{-x+y})!} \\\\ &amp; = \\frac{\\Gamma(x+\\alpha) \\Gamma(N-x+\\beta)}{(x-y)! (N-M-x+y)!} = (*) \\end{align*} \\] <p>The posterior calculation then gives a multiple of this when \\(x\\) is the only variable,</p> \\[ \\begin{align*} p(x|y) \\propto p(y|x)p(x) &amp; = \\frac{\\textcolor{red}{{x \\choose y}} \\textcolor{blue}{N-x \\choose M-y}} {N \\choose M} {N \\choose x} \\Gamma(x+\\alpha) \\Gamma(N-x + \\beta) \\\\ &amp; \\propto \\textcolor{red}{\\frac{x!}{(x-y!)}} \\textcolor{blue}{\\frac{(N-x)!}{(N-x-M+y)!}} \\frac{1}{x! (N-x)!} \\Gamma(x+\\alpha) \\Gamma(N-x + \\beta) \\\\ &amp;= \\frac{1}{(x-y)! (N-x-M+y)!} \\Gamma(x+\\alpha) \\Gamma(N-x + \\beta) \\\\ &amp; = (*) \\propto f(x-y|N-M, \\alpha+y, \\beta + M-y) \\end{align*} \\] <p>Reference other file here.</p>"},{"location":"mathematics/set_theory/universal_algebra/","title":"Universal Algebra","text":"<p>Based on the Johnstone notes.</p> <p>There is some good discussion on singleton sets here.</p> <p>Nullary Set</p> <p>We denote the nullary set as \\(G^0\\) as \\(\u2205\\)</p> <p>Finary Operation</p> <p>A finary operation is a map from a set \\(G\\) to any of $$ G^0, G^1=G, G^2 = G \\times G, ... $$</p> <p>Operational Type</p> <p>An operational type is a pair \\((\\Omega, \\alpha)\\) with</p> <ul> <li>set of operation symbols \\(\\Omega\\)</li> <li>a map \\(\\alpha : \\Omega \\mapsto \\mathbb{N}\\) that assigns each \\(\\omega \\in \\Omega\\) it's arity \\(\\alpha(\\omega)\\).</li> </ul> <p>Operational Structure</p> <p>An operational structure \\((\\Omega, \\alpha)\\) is</p> <ul> <li> <p>a set \\(A\\) with an operational type \\((\\Omega, \\alpha)\\)</p> </li> <li> <p>a function \\(\\omega_A : A^{\\alpha(\\omega)} \\mapsto A\\) for each \\(\\omega \\in \\Omega\\)</p> </li> </ul> <p>\\(\\omega_A\\) is the interpretation of abstract symbol \\(\\omega\\) in the structure \\(A\\).</p> <p>Homomorphism</p> <p>A map \\(f:A\\mapsto B\\) is a homomorphism of \\(\\Omega\\) structures \\(A\\) &amp; \\(B\\) if</p> \\[ f(\\omega_A (a_1,..., a_n)) = \\omega_B (f(a_1),...,f(a_n)) \\] <p>where \\(\\alpha(\\omega)=n\\) and \\(\\forall a_1,...,a_n \\in A\\)</p> <p>The set of \\(\\Omega\\)-terms in \\(X\\)</p> <p>Let \\(\\Omega\\) be an opertaional type and \\(X\\) be a set.</p> <p>The set \\(F_{\\Omega}(X)\\)  of \\(\\Omega\\) terms is defined inductively:</p> <ol> <li>If \\(x \\in X\\) then \\(x \\in F_{\u03a9}(X)\\)</li> <li>If \\(w \\in \\Omega\\) with \\(\\alpha(w)=n\\) and \\(t_1,...,t_n \\in \\Omega\\) then \\(wt_1...t_n \\in F_{\u03a9}(X)\\).</li> </ol> <p>Note that this really the intersection of all operation structures that contain \\(X\\).</p> <p>\\(F_\\Omega(X)\\) is a sigma structure</p> <ol> <li>The set of \\(\\Omega\\)-terms in \\(X\\), \\(F_{\\Omega}(X)\\) has a \\Omega structure.</li> <li>Given any \\(\\Omega\\) structure \\(F_\\Omega(X)\\) and any map \\(f:X \\mapsto A\\) there exists a unique homomorphism \\(\\hat{f}:F_\\Omega(X) \\mapsto A\\)</li> </ol> <p>In other words \\(F_\\Omega(X)\\) is the free structure generated by \\(X\\)</p> Proof of Theorem: \\(F_\\Omega(X)\\) is a sigma structure <p>Need to define the interpretation of \\(\\omega\\) on FX via $$ \\omega_{FX}(t_1,...,t_n) = \\omega t_1 ... t_n $$</p> <p>X terms from finite</p> <p>\\(F_{\\Omega}(X)=\\bigcup \\lbrace F_{\\Omega}(X^\\prime) \\mid X^\\prime \\subset X \\text{ is finite} \\rbrace\\)</p> <p>Derived Operations</p> <p>Let \\(X_n= \\lbrace x_1,...,x_n \\rbrace\\), \\(t \\in F_{\\Omega}(X_n)\\), \\(A\\) be a \\(\\Omega\\) structure.</p> <p>The n-ary derived operation corresponding to \\(t \\in F_{\\Omega}(X)\\), \\(t_A : A^n \\rightarrow A\\) is defined as</p> <p>\\(\\forall \\underline{a} \\in A^n\\)</p> \\[ t_A(\\underline{a}) = \\begin{cases}     a_i &amp;\\text{if } t = x_i \\in X_n \\\\     \\omega_A \\lparen t_1(\\underline{a}) ,..., t_m(\\underline{a}) \\rparen     &amp;\\text{if } t = \\omega t_1 ... t_m, \\text{ for } t_1,...,t_m \\in F_{\\Omega}(X_n), \\alpha(\\omega)=m \\end{cases} \\] <p>Note this really does just mean we keep applying the standard operations until we get to the X terms.</p> <p>Therefore a derived operation really is just the map it represents</p> <p>Derived Operationss are literally themselves</p> <p>If \\(t=mx_1 m x_2 x_3\\) then \\(t_A(\\underline{a})= m \\left( a_1, m \\left( a_2, a_3) \\right) \\right)\\)</p> <p>More generally if \\(t = \\dots \\omega x_1 \\dots x_m \\dots\\) then \\(t_A(\\underline{a}) = \\left( \\dots, \\omega \\left( a_1, \\dots a_m \\right), \\dots \\right)\\)</p> <p>In particular when \\(t \\in A = F_{\\Omega}(X_n)\\), \\(\\underline{u}=\\left( u_1,\\dots, u_n \\right)\\) $$ t_A(\\underline{u}) =  \\dots \\omega u_1 \\dots u_n \\dots = t[u_1/x_1, \\dots, u_n/x_n] \\ \\forall \\underline{u} \\in F_{\\Omega}(X_n) $$</p> <p>Where \\(t[u / x_i]\\) denote replacing symbol \\(x_i\\) with \\(u\\) where \\(x_i\\) appears in \\(t\\).</p> <p>This also means that $$ t_{F_{\\Omega}(X)}(x_1,\\dots, x_n) = t $$</p> <p>n-ary Equation</p> <p>An n-ary equation in operational type \\((\\Omega, \\alpha)\\) is an expression \\((s=t)\\) for \\(s,t \\in F_{\\Omega}(X_n)\\)</p> <p>Satisfied</p> <p>An n-ary equation is satisfied in operational structure A iff \\(t_A \\equiv s_A\\)</p> <p>Algebraic Theory</p> <p>An Algebraic Theory \\(T=(\\Omega, E)\\) is an operational type \\((\\Omega, \\alpha)\\) and set \\(E\\) of equations in \\(\\Omega\\)</p> <p>Model</p> <p>A model for an Algebraic Theory \\(T=(\\Omega, E)\\) or T-algebra is an Operational Structure \\(\\Omega\\) where all equations in \\(E\\) are satisfied.</p> <p>Group Theory Model</p> <p>A group \\(G\\) is a \\(\\left( \\omega, E \\right)\\) model where \\(\\Omega = \\{ m,i,e\\}\\) and \\(E=\\{(m x_1 m x_2 x_3 = m m x_1 x_2 x_3), (mex_1=x_1), (m i x_1 x_1 = e)\\}\\)</p> <p>Derived Equations</p> <p>Let \\(E\\) be a set of equations. The set \\(\\tilde{E}\\) of derived equation satisfies:</p> <ol> <li> <p>\\(E \\subset \\tilde{E}\\)</p> </li> <li> <p>\\(\\tilde{E}\\) defines an equivalence relation \\(s \\sim_E t \\iff (s=t) \\in \\tilde{E}\\) on the set of terms \\(F_{\\Omega}(X)\\)</p> <ol> <li>reflexive: \\((t=t) \\in \\tilde{E} \\ \\forall t \\in F_{\\Omega}(X)\\)</li> <li>symmetric: \\((s=t) \\implies (t=s)\\)</li> <li>transitive: \\((s=t),(t=u) \\implies (s=u)\\)</li> </ol> </li> <li> <p>Substitution rules:</p> <ol> <li>\\((s=t) \\in \\tilde{E} \\implies (s[u / x_i] = t[u / x_i])\\)</li> <li>\\((s=t), u \\in F_{\\Omega}(X_n) \\implies u[s/x_i] = u[t/x_i]\\)</li> </ol> </li> </ol> <p>This is just the smallest subset of all possible equations \\((s=t)\\) such that it is closed under a,b,c.</p> <p>Group Theory Example</p> <ol> <li>\\(t = m e x_1 = x_1 = s \\in \\tilde{E} \\implies m e u = u \\forall u \\in F_{\\Omega}(X)\\)</li> <li>\\(u = m x_1 x_2\\), \\(t=s\\) as before, \\(\\implies m x_1 \\textcolor{red}{mex_1} = m x_1 \\textcolor{red}{x_1}\\)</li> </ol> <p>Equivalence Classes of Equations on X</p> <p>The equivalence classes of \\(\\sim_E\\) on \\(F_{\\Omega}(X)\\) is \\(F_{(\\Omega, E)}(X)\\)</p> <p>The set of equivalence classes satisfies the equations in E</p> <ol> <li> <p>\\(F_{(\\Omega, E)}(X)\\) inherits \\(\\Omega\\)-structure from \\(F_{\\Omega}(X)\\) and satisfies the equations in \\(E\\).</p> </li> <li> <p>\\(F_{(\\Omega, E)}(X)\\) is the free model generated by X.</p> <p>i.e. For any homomorphism \\(f: X \\to (\\Omega, E) \\text{ model } A\\) there is a unique homomorphism \\(\\tilde{f}: F_{(\\Omega, E)}(X) \\to A\\)</p> </li> </ol> <p>Let \\([t]\\) denote the equivalence class of \\(t \\in F_{\\Omega}(X)\\)</p> Proof of Theorem: Equivalence classes satisfy E <p>Part I</p> <p>We need to show that this indeed an \\(\\Omega\\) structure. This requires showing that the interpretation of \\(\\Omega\\) operations in \\(F_{\\Omega}(X)\\) respects the equivalence relation.</p> <p>Let \\(FX=F_{\\Omega}(X)\\) and \\(EX=F_{(\\Omega, E)}(X)\\).</p> <p>In other words we need to show that the interpretation \\(\\omega_{EX}(\\underline{u})=[\\omega_{FX}(\\underline{u})]\\) is well defined.</p> <p>Consider operation \\(\\omega \\in \\Omega\\) with \\(\\alpha(\\omega)=n\\) and symbols \\(t_1,\\dots, t_n\\) and \\(s_1, \\dots, s_n\\) where \\(t_i \\sim_{E} s_i\\).</p> <p>We need to show that the operations result is the same regardless of the representative picked for each equivalence class:</p> \\[ \\begin{align*} \\text{[definition] } \\omega([t_1], \\dots, [t_n]) &amp;= &amp;\\omega [t_1] \\dots [t_n] \\\\ \\text{[substitution rule 3b]} &amp; \\sim_E &amp;\\omega [s_1] \\dots [t_n] \\\\ &amp;\\vdots \\\\ \\text{[substitution rule 3b]} &amp;\\sim_E &amp;\\omega [s_1] \\dots [s_n] \\\\ \\text{[definition]} &amp;= &amp;\\omega([s_1],\\dots,[s_n]) \\end{align*} \\] <p>Hence we have a valid interpretation that respects the equivalence relation.</p> <p>It then follows by induction that the derived operations also have a well defined interpretation.</p> <p>Let \\(t=\\omega t_1 \\dots t_m\\), then for any \\(\\underline{u} \\in FX^n\\)</p> \\[ \\begin{align*} \\text{[definition of derived operation] } t_{EX} \\left( \\underline{u} \\right) &amp;= \\omega_{EX} \\left( {t_1}_{EX} \\left( \\underline{u} \\right) ,\\dots, {t_m}_{EX} \\left( \\underline{u} \\right) \\right) \\\\ \\text{[inductive hypothesis] } &amp;= \\omega_{EX} \\left( [{t_1}_{FX} \\left( \\underline{u} \\right)] ,\\dots, [{t_m}_{FX} \\left( \\underline{u} \\right)] \\right) \\\\ \\text{[definition of operations in } EX \\text{] } &amp;= \\left[ \\omega_{FX} \\left({t_1}_{FX} \\left( \\underline{u} \\right) ,\\dots, {t_m}_{FX} \\left( \\underline{u} \\right) \\right) \\right] \\\\ &amp;= [t_{FX}(\\underline{u})] \\end{align*} \\] <p>Hence \\(t_{EX}(\\underline{u}) = [t_{FX}(\\underline{u})]\\). \\((\\text{**})\\)</p> <p>It is kind of immediate that this follow by induction. I wanted to be really verbose.</p> <p>The next part we need to show is that all equations in \\(E\\) are satisfied.</p> <p>Consider equation \\(\\left( s=t \\right) \\in E\\) where \\(s,t \\in F = F_{\\Omega}(X_n)\\).</p> <p>We want to show that this equation is satisfied in \\(EX=F_{(\\Omega, E)}(X)\\), i.e \\(s_{EX}=t_{EX}\\)</p> <p>For any \\(\\underline{u} \\in FX^n\\)</p> \\[ \\begin{align*} \\text{[by (**)] } s_{EX}(\\underline{u}) &amp;= \\left[ s_{FX} (\\underline{u}) \\right] \\\\ \\text{[see example]} &amp;= \\left[ s\\left[ u_1/x_1, \\dots u_n/x_n \\right] \\right] \\\\ \\text{[substitution 3a repeated } n \\text{ times]} &amp;= \\left[ t\\left[ u_1/x_1,\\dots,u_n/x_n \\right] \\right] \\\\ &amp;= \\left[ t_{FX} \\left( \\underline{u} \\right) \\right] \\\\ &amp;= t_{EX} \\left( \\underline{u} \\right) \\end{align*} \\] <p>Part II</p> <p>Let \\(\\hat{E} = \\left\\{ \\left( s=t \\right) : h(s)=h(t) \\text{ for any homomorphism } h: F_{\\Omega}(X) \u2192 \\left( \\Omega, E \\right) -\\text{Model } A \\right\\}\\)</p> <p>Let's recall some facts:</p> <ul> <li> <p>All homomorphisms from \\(F_{\\Omega}(X)\\) to \\(A\\) just fill in the symbols \\(x_1,\\dots x_n\\) from template \\(t \\in F_{\\Omega}(X_n)\\) with some choice of \\(a_1, \\dots, a_n\\).</p> </li> <li> <p>Any homomorphism from \\(F_{\\Omega}(X)\\) to some structure \\(A\\) is uniquely defined by \\(h(x_i)\\) for \\(i=1,\\dots,n\\)</p> </li> </ul> <p>We shall show that the closure properties of \\(\\tilde{E}\\) definition are satisfied.</p> <p>Definition part 1.</p> <p>It is kind of obvious that \\(s=s_{FX} \\left( x_1, \\dots, x_i, \\dots, x_n \\right)\\).</p> <p>Suppose \\(s=t \\in \\tilde{E}\\) then</p> \\[ \\begin{align*} h(s) &amp;= h(s_{FX}(x_1, \\dots, x_n)) \\\\ &amp;= s_A \\left( h(x_1), \\dots, h(x_n) \\right) \\\\ [\\text{equations in } \\tilde{E} \\text{ are true for model } A] &amp;= t_A \\left( h(x_1), \\dots, h(x_n) \\right) \\\\ &amp;= h(t) \\end{align*} \\] <p>Definition part 2 RST is easy.</p> <p>Definition part 3a.</p> <p>Fix \\(i\\), let \\(\\left( s=t \\right) \\in \\hat{E}\\), \\(h\\) be some homomorphism from \\(F_{\\Omega}(X)\\) to \\(A\\) and \\(h'\\) be the uniquely defined homomorphism that sends \\(x_i\\) to \\(h(u)\\) and \\(x_j\\) to \\(h(x_j)\\) for \\(i \\neq j\\).</p> <p>It is also kinda obvious that \\(s[u/x_i]= s_{FX} \\left( x_1,\\dots,u,\\dots x_n   \\right)\\).</p> <p>Therefore,</p> \\[ \\begin{align*} h \\left( s \\left[ u/x_i \\right] \\right) &amp;= h \\left( s_{FX} \\left( x_1,\\dots,u,\\dots,x_n \\right) \\right) \\\\ \\text{[homomorphism]} &amp;= s_{A} \\left( h(x_1), \\dots, h(u), \\dots, h(x_n) \\right) \\\\ &amp;= s_{A} \\left( h'(x_1), \\dots, h'(x_i), \\dots h'(x_n) \\right) \\\\ \\text{[homomorphism]} &amp;= h' \\left( s_{FX} \\left( x_1,\\dots,x_i,\\dots,x_n \\right) \\right) \\\\ &amp;= h'(s) \\\\ \\text{[s and t agree on any homomorphism]} &amp;= h'(t) \\\\ \\dots \\\\ &amp;= h \\left( t \\left[ u/x_i \\right] \\right) \\end{align*} \\] <p>Definition part 3b.</p> <p>Given \\(s=t \\in \\hat{E}\\) we want to show that \\(h \\left( u \\left[ t/x_i \\right] \\right) = h \\left( u \\left[ s/x_i \\right] \\right)\\)</p> \\[ \\begin{align*} h \\left( u \\left[ t/x_i \\right] \\right) &amp;= h \\left( u_{FX} \\left( x_1, \\dots, t, \\dots, x_n \\right) \\right) \\\\ &amp;= u_A \\left( h(x_1), \\dots, h(t), \\dots, h(x_n) \\right) \\\\ [ \\text{since }h(s)=h(t)] &amp;= u_A \\left( h(x_1), \\dots, h(s), \\dots, h(x_n) \\right) \\\\ &amp;= h \\left( u \\left[ t/x_i \\right] \\right) \\end{align*} \\] <p>By satisfying the definition we have shown that \\(\\tilde{E} \\subseteq \\hat{E}\\). This means that if we take elements \\(s \\in F_{\\Omega}(X)\\) and factor then through the quotient map to \\([s] \\in F_{(\\Omega, E)}(X)\\) we can define \\(\\tilde{f}\\) as \\(\\hat{f}\\) applied to any representative of \\([s]\\).</p> <p>In particular let $\\hat{f}: F_{\\Omega}(X) \u2192 A $ be the unique homomorphism extending homomorphism \\(f: X \u2192 A\\), defined in previous theorem.</p> <p>Define \\(\\tilde{f}: F_{(\\Omega, E)}(X) \u2192 A\\) by \\(\\tilde{f}([s]) = \\hat{f}(s)\\). This is a uniquely defined homomorphism that extends \\(f\\).</p>"}]}