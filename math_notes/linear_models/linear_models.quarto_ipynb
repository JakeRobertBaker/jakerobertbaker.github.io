{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Linear Model Notes\"\n",
        "author: \"Jake Baker\"\n",
        "toc: true\n",
        "number-sections: true\n",
        "highlight-style: pygments\n",
        "format: \n",
        "  html: \n",
        "    html-math-method: katex\n",
        "diagram:\n",
        "  cache: true\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## OLS Basics\n",
        "\n",
        "::: {#def-standard_normal}\n",
        "A random vector $\\mathbf{z} \\in \\mathbb{R^n}$ is standard normal **iff** components \n",
        "$(\\mathbf{z_i})_{i=1}^n$\n",
        "are independantly identically distributed $\\mathcal{N}(\\mathbf{0,I_n})$.\n",
        ":::\n",
        "\n",
        "::: {.callout-note}\n",
        "If a vector in standard normal w.r.t one basis then it is standard normal w.r.t any basis.\n",
        ":::\n",
        "\n",
        "::: {#def-ols_lm}\n",
        "## OLS\n",
        "An ordinary least squares linear model (OLM) provides an estimate $\\hat{\\beta}$ of unknown coefficent $\\beta \\in \\mathbb{R^{p \\times 1}}$ to the problem,\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = \\mathbf{X} \\beta + \\mathbf{\\varepsilon}\n",
        "$$\n",
        "\n",
        "that minimises $\\| \\hat{\\varepsilon} \\|$ for $\\hat{\\varepsilon} = \\mathbf{y-X \\hat{\\beta}}$.\n",
        "\n",
        "Items $\\mathbf{y} \\in \\mathbb{R}^{n \\times 1}, \\mathbf{X} \\in \\mathbb{R^{n \\times p}}$ are known.\n",
        "\n",
        "We **assume** that $\\dfrac{\\varepsilon}{\\sigma}$ is\n",
        "standard normal (@def-standard_normal)\n",
        "$\\Leftrightarrow \\varepsilon \\sim \\mathcal{N}(\\mathbf{0, \\sigma^2 I_n})$,\n",
        "where we may not know $\\sigma$.\n",
        ":::\n",
        "\n",
        "::: {#thm-ols_solution}\n",
        "## Solution of OLS\n",
        "Assume that $\\mathbf{X}$ has full rank.\n",
        "\n",
        "The solution of OLS is given by $\\hat{\\beta} = \\mathbf{(X^TX)^{-1}X^Ty}$.\n",
        ":::\n",
        "\n",
        "\n",
        "::: {.proof}\n",
        "Let subspace $U \\leq \\mathbb{R^n}$ generated via the span of $\\mathbf{X}$ columns. Then by property of orthogonal projection the \n",
        "$\\mathbf{\\hat{y}=X \\beta} \\in U$\n",
        "that minimises \n",
        "$\\| \\hat{\\varepsilon} \\| = \\| \\mathbf{y-\\hat{y}} \\|$ is given by $\\mathbf{P}_U \\mathbf{y}$.\n",
        "\n",
        "Therefore,\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\forall u \\in U,\n",
        "\\langle u, \\hat{\\varepsilon}\\rangle =0 &\\implies \\mathbf{(y-X \\hat{\\beta})^TX} = 0\n",
        "\\\\\n",
        "&\\Leftrightarrow\n",
        "\\mathbf{y^T X} = \\mathbf{\\hat{\\beta}^T X^TX}\n",
        "\\\\\n",
        "&\\Leftrightarrow\n",
        "\\mathbf{X^Ty} = \\mathbf{X^T X \\hat{\\beta}}\n",
        "\\\\\n",
        "&\\Leftrightarrow\n",
        "\\mathbf{\\hat{\\beta}} = \\mathbf{(X^TX)^{-1}X^Ty}\n",
        "\\end{align*}\n",
        "$$\n",
        ":::\n",
        "\n",
        "\n",
        "## Large Model vs Submodel\n",
        "\n",
        "::: {#thm-anova_F_test}\n",
        "## ANOVA F-test\n",
        "Let $p=p_0+p_1, \\mathbf{X = [X_0, X_1]} \\in \\mathbb{R}^{n \\times p}$ for \n",
        "$\\mathbf{X_0} \\in \\mathbb{R}^{n \\times p_0}, \\mathbf{X_1} \\in \\mathbb{R}^{n \\times p_1}$\n",
        "\n",
        "Consider the following two hypothesis of a full model vs a submodel,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\textbf{H}_0 \\text{ (null)}&:\n",
        "\\mathbf{y = X_0 \\beta_\\mathcal{N} + \\varepsilon},\n",
        "\\quad\n",
        "&& \\mathbf{\\beta}_\\mathcal{N} \\in \\mathbb{R}^{p_0 \\times 1},\n",
        "\\quad\n",
        "&&&\\mathbf{\\varepsilon \\sim \\mathcal{N}(0,I_n)}\n",
        "\\\\\n",
        "\\textbf{H}_1 \\text{ (full)}&:\n",
        "\\mathbf{y = X \\beta_\\mathcal{F} + \\varepsilon},\n",
        "\\quad\n",
        "&& \\mathbf{\\beta}_\\mathcal{F} \\in \\mathbb{R}^{p \\times 1},\n",
        "\\quad\n",
        "&&&\\mathbf{\\varepsilon \\sim \\mathcal{N}(0,I_n)}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Under the null hypothesis the following statistic is F distributed.\n",
        "$$\n",
        "\\dfrac{\\| \\mathbf{\\hat{\\varepsilon}_\\mathcal{N} - \\hat{\\varepsilon}_\\mathcal{F}} \\| /p_1}\n",
        "{\\| \\mathbf{\\hat{\\varepsilon}_\\mathcal{F}} \\|/(n-p)}\n",
        "\\sim\n",
        "F_{p_1, n-p}\n",
        "$$\n",
        ":::\n",
        "\n",
        "<!-- start proof -->\n",
        "::: {.proof}\n",
        "The following picture is helpful:\n",
        "\n",
        "::: {#fig-anova_plot}\n",
        "![ANOVA Image](geometry_lss_extended.png)\n",
        "ANOVA PLot\n",
        "\n",
        "Visualisation of the geometry of the null and full model.\n",
        ":::\n",
        "\n",
        "Let $U_0, U$ denote the column span of $\\mathbf{X_0, X}$ respectively. Since $U_0 \\leq U$ the orthogonal compliment of $U_0$ restricted to $U$ is in the direct sum $U = U_0 \\oplus U_0^{\\perp|_U}$. Also $V = U \\oplus U^\\perp$. Therefore $V$ is the direct sum of orthogonal subspaces $U_0, U_1, U^\\perp$, for $U_1 = U_0^{\\perp|_U}$.\n",
        "\n",
        "Since the components of $\\mathbf{\\varepsilon}$ are independant w.r.t any normal basis it olds that it holds that projections $\\mathbf{P}_{U_0} \\varepsilon, \\mathbf{P}_{U_1} \\varepsilon, \\mathbf{P}_{U^\\perp} \\varepsilon$ are independant because each projection can be expressed in terms of disjoint elements of the orthnormal basis created by combining orthonormal basis of $U_0, U_1, U^\\perp$.\n",
        "\n",
        "We will now show that \n",
        "$\\mathbf{\\hat{\\varepsilon}_\\mathcal{N} - \\hat{\\varepsilon}_\\mathcal{F}} = \\mathbf{P}_{U_1} \\varepsilon$\n",
        "and\n",
        "$\\mathbf{\\hat{\\varepsilon}_\\mathcal{F}} = \\mathbf{P}_{U^\\perp} \\varepsilon$. This implies that they are independant normal and therefore the ratio of their norms divided by their dimension/degrees of freedom is eqivalent to an F distribution and thus completing the proof.\n",
        "\n",
        "We see,\n",
        "\n",
        "$\\mathbf{y} = \\mathbf{P}_U \\mathbf{y} + \\mathbf{P}_U^\\perp \\mathbf{y}$\n",
        "from directness,\n",
        "\n",
        "$\\mathbf{y} = \\mathbf{P}_U \\mathbf{y} + \\mathbf{\\hat{\\varepsilon}}_\\mathcal{F}$\n",
        "from OLS definition and the fact that full fitted \n",
        "$\\mathbf{y}$ is given by $\\mathbf{P}_U \\mathbf{y}$\n",
        "\n",
        "Hence \n",
        "$\\mathbf{\\hat{\\varepsilon}}_\\mathcal{F} = \\mathbf{P}_{U^\\perp} \\mathbf{y}$. Also,\n",
        "\n",
        "$\\mathbf{y} = \\mathbf{P}_{U_0} \\mathbf{y} + \\mathbf{P}_{U_1} \\mathbf{y} + \\mathbf{P}_{U^\\perp} \\mathbf{y}$ \n",
        "from directness,\n",
        "\n",
        "$\\mathbf{y} = \\mathbf{P}_{U_0} \\mathbf{y} + \\mathbf{\\hat{\\varepsilon}}_\\mathcal{N}$\n",
        "from OLS definition and the fact that null fitted \n",
        "$\\mathbf{y}$ is given by $\\mathbf{P}_{U_0} \\mathbf{y}$\n",
        "\n",
        "Hence \n",
        "$\\mathbf{\\hat{\\varepsilon}}_\\mathcal{N} = \\mathbf{P}_{U_1} \\mathbf{y} + \\mathbf{P}_{U^\\perp }\\mathbf{y}$.\n",
        "Therefore\n",
        "$\\mathbf{\\hat{\\varepsilon}}_\\mathcal{N} - \\mathbf{\\hat{\\varepsilon}}_\\mathcal{F} = \\mathbf{P}_{U_1} \\mathbf{y}$.\n",
        "\n",
        "Now under the null hypothesis $\\mathbf{y} = \\mu_\\mathcal{N} + \\varepsilon$ for some $\\mu_\\mathcal{N} \\in U_0$, therefore since $U_1, U^\\perp$ are orthogonal to $U_0$ it holds that for any\n",
        "$W \\in \\set{U_1, U^\\perp} \\ \\mathbf{P}_W \\mathbf{y} = \\mathbf{P}_W \\mathbf{\\varepsilon}$. This gives us what we wanted.\n",
        "<!-- end proof -->\n",
        ":::\n",
        "\n",
        "## R2 and Sample Correlation\n",
        "\n",
        "::: {#def-sample_correlation}\n",
        "The sample correlation between vectors $\\mathbf{x,y}$ is defined as\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "r(\\mathbf{x,y}) \n",
        "&=\n",
        "\\cfrac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{ \\sqrt{\\sum_i (x_i - \\bar{x})^2}  \\sqrt{\\sum_i (y_i - \\bar{y})^2}}\n",
        "\\\\ &=\n",
        "\\cfrac{(\\mathbf{x-P_1 x})^T (\\mathbf{y-P_1 y})}{\\| \\mathbf{x-P_1 x} \\| \\| \\mathbf{y-P_1 y} \\|}\n",
        "\\\\ &=\n",
        "\\bigg\\langle\n",
        "  \\cfrac{\\mathbf{P_{1^\\perp} x}}{\\|\\mathbf{P_{1^\\perp} x}\\|},\n",
        "  \\cfrac{\\mathbf{P_{1^\\perp} y}}{\\|\\mathbf{P_{1^\\perp} y}\\|}\n",
        "\\bigg\\rangle\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Where we are lax with notation and let $\\mathbf{1}$ denote $\\text{span}\\set{\\mathbf{1}}$ for \n",
        "$\\mathbf{1} = [1,\\ ...\\ ,1]^T$.\n",
        ":::\n",
        "\n",
        "\n",
        "::: {#def-R2}\n",
        "The data and fitted values $\\mathbf{y}$ and $\\mathbf{\\hat{y}}$, the **coefficent of determination** is defined as $R2(\\mathbf{y,\\hat{y}})$.\n",
        "$$\n",
        "\\begin{align*}\n",
        "&\n",
        "R^2 = 1 - \\frac{SS_{res}}{SS_{tot}},\n",
        "& \\text{for }\n",
        "SS_{res} = \\| \\mathbf{y-\\hat{y}} \\|^2\n",
        "& &\n",
        "SS_{tot} = \\| \\mathbf{y-\\bar{y}} \\|^2\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```{tex}\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        ":::"
      ],
      "id": "df8905e0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}